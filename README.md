# AGENTIC AI RAG FOR GOOGLE DRIVE STORAGE

![RAG LLM](https://img.shields.io/badge/RAG%20LLM-lightgrey?style=flat)
![LangChain](https://img.shields.io/badge/LangChain-4285F4?logo=google-cloud&logoColor=white&style=flat)
![Google Drive API](https://img.shields.io/badge/Google%20Drive%20API-4285F4?logo=googledrive&logoColor=white&style=flat)
![GCP](https://img.shields.io/badge/GCP--4285F4?logo=google-cloud&logoColor=white&style=flat)
[![Chroma](https://img.shields.io/badge/Chroma-1DB954?style=flat&logo=appveyor&logoColor=white)](https://www.trychroma.com)

---
![app logo](assets/images/RagApp.png)

# APP connections to GOOGLE CLOUD PLATFORM and others for app needs
For just this project we need to connect GCP and to be able to edit credentials, enable/disable APIs
We need also create all the best rights and secured data access for our users.


# About App and Users data
## Create a project and join a service account to with editor role
## Enable Google DRIVE API
L'API Drive n√©cessite des autorisations dans la console Google Cloud et lorsque vous lancez l'API dans votre code.
Dans la console Google Cloud, vous devez d√©clarer les autorisations dont votre application a besoin dans la configuration de son √©cran de consentement OAuth. Il s'agit du niveau d'autorisation le plus √©lev√© que votre application puisse demander. Il s'agit d'une demande formelle adress√©e √† Google. Les niveaux d'acc√®s d√©clar√©s sont ceux que Google affiche aux utilisateurs sur l'√©cran de consentement. Il permet √† l'utilisateur de comprendre exactement √† quelles donn√©es et actions votre application demande l'acc√®s. Pour en savoir plus, [consultez Configurer l'√©cran de consentement OAuth et choisir des habilitations](https://developers.google.com/workspace/guides/configure-oauth-consent?hl=fr).
# Create OAuth2 for client ID and Client Secret that will allow users authentication to App.

OAuth2 is used for apps that have to access to user data like Google Drive file, or even insert data to google sheet database.

## How OAuth2 operates ?
![Google Drive relationships Diagram](https://developers.google.com/static/workspace/drive/images/drive-intro.svg?hl=fr)

This how users connect to your Google Drive App.

At first set up Google Auth Platform


**Rappel**:

Pour l'application :

1- Lire les fichiers Drive de l‚Äôutilisateur

2- Cr√©er des chunks et embeddings (c√¥t√© app)

3- Stocker ces embeddings dans une base vectorielle (c√¥t√© app, pas dans Drive)

4- Pas de cr√©ation, modification ou suppression de fichiers dans Drive.


Les scopes dont j'aurai probablement besoin:

üîé Analyse des scopes

Scopes|roles
--|--
https://www.googleapis.com/auth/drive.readonly | ‚úÖ Tu pourras lire tout le contenu des fichiers Drive (donc ouvrir et traiter les fichiers).
https://www.googleapis.com/auth/drive | Acc√®s complet (lecture/√©criture/suppression) üö´ Trop large pour ton besoin, et surtout √ßa d√©clenchera une v√©rification de s√©curit√© obligatoire chez Google si tu veux ouvrir ton app √† des utilisateurs externes.



Il faut les d√©finir √† la fois c√¥t√© Google console  et cot√© Application


D√©claration des scopes

Google Console | Google APP (ici mon app qui se connecte √† google)
-|-
(afficher sur la page de consentement) | (Quand tu lances ton script, l‚Äôutilisateur est redirig√© vers la page d‚Äôautorisation. Cette page affiche les scopes que tu as demand√©s ‚Üí Google compare avec ceux d√©clar√©s dans la console.). Si √ßa correspond, l‚Äôutilisateur peut accepter ‚Üí ton app re√ßoit un token d‚Äôacc√®s avec exactement ce scope.


‚úÖ En r√©sum√© :

Console Google : d√©claration officielle des scopes que ton app utilisera (pour l‚Äô√©cran de consentement).

Code Python : scopes que tu demandes r√©ellement quand tu cr√©es le **flux OAuth**.

Les deux doivent matcher.


**D√©finition des scopes dans Google Console**
![google scopes for app behavior](assets/images/scopes_categories.png)

****
[Basic app verification required](https://support.google.com/cloud/answer/13463073?visit_id=638942145643558756-1868041267&hl=fr&rd=1&dark=1&sjid=10812633968201329191-EU#ver-prep&zippy=%2Csteps-to-prepare-for-verification)


**Centre de validation**

Google, pour cette app, doit v√©rifier l'acc√®s aux donn√©es.(but est de v√©rifier si l'app)

üèÅ Comment se pr√©sente la validation

Si tu cliques sur ‚ÄúDemander une validation‚Äù, Google te lance dans un process qui ressemble √† √ßa :

1. Informations obligatoires :

URL de politique de confidentialit√© (m√™me un site simple type GitHub Pages ou Notion).

Nom et logo de ton application.

Adresse e-mail de contact.

2. Preuves d‚Äôusage du scope sensible :

Tu dois expliquer pourquoi tu as besoin du scope drive.readonly.

D√©montrer que tu n‚Äôutilises pas les donn√©es √† mauvais escient.

3. Vid√©o de d√©mo :

Tu dois tourner une vid√©o qui montre comment l‚Äôutilisateur interagit avec ton app, comment il autorise l‚Äôacc√®s, et ce que ton app fait avec les fichiers.

4. Revue Google :

Ils v√©rifient que ton app correspond bien √† la description.

Ils peuvent demander des pr√©cisions.

D√©lai : quelques jours √† quelques semaines selon la complexit√©.

[Pour plus d'informations](https://support.google.com/cloud/answer/15549049?hl=fr&visit_id=638942145644745956-18931629&rd=1#verification-status&zippy=%2Capp-name) ...


# Install the project
Be aware that all things explained above are a MUST.

```
git clone https://github.com/donat-konan33/GoogleDriveAgenticIARag.git
cd GoogleDriveAgenticIARag
```

# About project, tools, prompts, Database and RAG Pipeline used

![chroma db](https://docs.trychroma.com/_next/image?url=%2Fcomputer-dark.png&w=1920&q=75)

Chroma is the open-source AI application database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs.

According to the above operating figure, ``chroma db`` can rephrase the query emitted by users.

Chroma gives you everything you need for retrieval:

- Store embeddings and their metadata
- Vector search
- Full-text search
- Document storage
- Metadata filtering
- Multi-modal retrieval




üîπ ***Comprendre la recherche vectorielle***

Il s'agit de cr√©er espace vectorielle des donn√©es et de comparer leur similarit√© √† celle d'un vecteur correspondant √† une requ√™te (Example: la question √† un chatbot est transform√©e en vecteur via la technique d'embeddings)


1. **Documents ‚Üí Chunks**

- Un document est d√©coup√© en petits morceaux (chunks), souvent des phrases, paragraphes ou segments.

- Chaque chunk contient un texte limit√© pour √™tre trait√© par un mod√®le d‚Äôembedding.

2. **Chunks ‚Üí vecteurs**

- Chaque chunk est transform√© en vecteur num√©rique √† l‚Äôaide d‚Äôun mod√®le d‚Äôembeddings (ex : OpenAI, SentenceTransformers).

- Le vecteur capture le sens s√©mantique du texte, pas juste les mots exacts.

3. **Vecteurs ‚Üí espace vectoriel**

- Tous les vecteurs sont plac√©s dans un espace vectoriel multidimensionnel (souvent 512 √† 1536 dimensions selon le mod√®le).

- Chaque vecteur repr√©sente un chunk et sa position refl√®te sa similarit√© avec les autres :

    - Vecteurs proches ‚Üí chunks similaires.

    - Vecteurs √©loign√©s ‚Üí chunks diff√©rents.

4. **Recherche vectorielle**

- Quand on fait une requ√™te, elle est aussi transform√©e en vecteur.

- On calcule la distance ou similarit√© entre ce vecteur de requ√™te et les vecteurs des chunks.

- Les chunks les plus proches dans l‚Äôespace vectoriel sont retourn√©s comme r√©sultats pertinents.

üîπ **Points cl√©s**

- Les chunks permettent de g√©rer les documents volumineux et d‚Äôavoir des r√©sultats pr√©cis.
- L‚Äôespace vectoriel est la base de la recherche s√©mantique : il permet de retrouver du contenu similaire m√™me si les mots exacts ne sont pas utilis√©s.
- L‚Äôapproche est utilis√©e dans des syst√®mes comme ChatGPT retrieval-augmented generation (RAG), moteurs de - recherche intelligents, et assistants documentaires.


```
                                    Document source
                                           ‚îÇ
                                           ‚ñº
                                    D√©coupage en chunks
                                           ‚îÇ
                                           ‚ñº
                                    Vecteurs d‚Äôembeddings
                                           ‚îÇ
                                           ‚ñº
                                    Index vectoriel (Chromadb, FAISS, Milvus‚Ä¶)
                                           ‚îÇ
                                           ‚ñº
                                    Requ√™te utilisateur
                                           ‚îÇ
                                           ‚ñº
                                    Vecteur de la requ√™te (li√© √† la requ√™te du client)
                                           ‚îÇ
                                           ‚ñº
                                    Recherche des vecteurs proches ()
                                           ‚îÇ
                                           ‚ñº
                                    Chunks pertinents ‚Üí R√©sultats ou RAG
```


## Principes et √©tapes cl√©s de la vectorisation


1. **Texte brut**

Commence avec ton texte ou ton chunk de document :

"Bonjour, ceci est un exemple de texte."


2. **Tokenisation**

- Transformer le texte en tokens (unit√©s de base que le mod√®le comprend).
- Exemple : "Bonjour, ceci est un exemple" ‚Üí ["Bonjour", ",", "ceci", "est", "un", "exemple"]
- Ajout de tokens sp√©ciaux selon le mod√®le ([CLS], [SEP], padding).
- AutoTokenizer est pratique : il d√©tecte automatiquement le tokenizer correspondant au mod√®le choisi.

3. **Encodage en tenseurs**

- Les tokens sont convertis en IDs num√©riques et en tenseurs PyTorch ou TensorFlow.
- Padding/truncation pour que tous les chunks aient la m√™me longueur si n√©cessaire.

4. **Passage dans le mod√®le**

- Le mod√®le transforme les tokens en repr√©sentations vectorielles (embeddings).
- Parfois, on r√©cup√®re :
    -     last_hidden_state ‚Üí vecteurs pour chaque token
    -     pooler_output ou moyenne ‚Üí vecteur global du texte

5. **Post-traitement (optionnel)**

- Moyenne des vecteurs token pour obtenir embedding global du chunk.
- Normalisation (ex. L2 normalization) si on veut calculer cosine similarity.

6. **Indexation**

- Stocker l‚Äôembedding dans un index vectoriel (FAISS, Milvus, Pinecone‚Ä¶).
- Chaque vecteur est li√© √† son chunk/document d‚Äôorigine.

**R√¥le de AutoTokenizer**

AutoTokenizer est toujours pertinent si :
- Tu veux charger un mod√®le Hugging Face sans te soucier du tokenizer exact (BERT, MPNet, etc.).
- Il choisit automatiquement le tokenizer compatible avec ton mod√®le (vocabulaire, tokens sp√©ciaux, normalisation).

**Alternative** : tu peux instancier directement un tokenizer sp√©cifique (MPNetTokenizer) si tu veux un contr√¥le fin, mais pour la plupart des cas AutoTokenizer suffit et simplifie le code.

```
                                        Texte brut
                                            ‚îÇ
                                            ‚ñº
                                        Tokenisation (AutoTokenizer)
                                            ‚îÇ
                                            ‚ñº
                                        Conversion en tenseurs
                                            ‚îÇ
                                            ‚ñº
                                        Mod√®le (AutoModel / SentenceTransformer)
                                            ‚îÇ
                                            ‚ñº
                                        Vecteur (embedding du chunk)
                                            ‚îÇ
                                            ‚ñº
                                        Index vectoriel / recherche vectorielle
```

![diagram depictment](https://learn.microsoft.com/en-us/data-engineering/playbook/images/intro_file_processing.png)


## Vector Database Chroma
Vector databases provide essential production capabilities:

- Persistent storage: Data survives system restarts and crashes
- Optimized indexing: Fast similarity search using HNSW algorithms
- Memory efficiency: Handles millions of vectors without RAM exhaustion
- Concurrent access: Multiple users query simultaneously
- Metadata filtering: Search by document properties and attributes
[ChromaDB](https://github.com/chroma-core/chroma) delivers these features with a Python-native API that integrates seamlessly into your existing data pipeline.

## Choice of the best Open-source llm fro our project
I choose according to for relevant factors such as :
- Open-Source version
- Usage Type (Rag, QA, chatbot)
- generation and text understanding
- translator, summarizer

We can OpenLLM API to execute a model from whether local or remote server. Once install there are all OpenAI-BAsed models that can be used.

Our project is created to be used by french. So Mistral 7B quantified could be the best choice for our application based on its main goal.

For more details hit this [link](https://python.langchain.com/docs/integrations/llms/openllm/) leading you forward the langchain page talking about that.

| Crit√®re             | Chatbot ü§ñ                    | QA üîç                                 |
| ------------------- | ----------------------------- | ------------------------------------- |
| **But**             | Dialogue naturel              | R√©ponse factuelle                     |
| **Contexte**        | Historique de la conversation | Base documentaire externe             |
| **Fiabilit√©**       | Peut halluciner               | Plus fiable (car fond√© sur documents) |
| **Exemple d‚Äôusage** | Assistant client, discussion  | Recherche juridique, scientifique     |

## Langchain Components
[Chat models](https://python.langchain.com/docs/integrations/chat/)
[Retrievers](https://python.langchain.com/docs/integrations/retrievers/)
[Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)
[Document loaders webpages - crawl](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html)
[Document loaders webpages - web](https://python.langchain.com/docs/integrations/document_loaders/web_base/)

[Vector stores](https://python.langchain.com/docs/integrations/vectorstores/)
[Embedding models](https://python.langchain.com/docs/integrations/text_embedding/)
[Gemini embeddings model](https://ai.google.dev/gemini-api/docs/embeddings)
[model caches](https://python.langchain.com/docs/integrations/llm_caching/)

Chat Loaders : Telegram, WhatsApp, WeChat, Slack, twitter(via Apify), LangSmith Chat Datasets, LangSmith LLM Runs

**LangSmith**
üéØ Ce que fait LangSmith

- D√©bogage et tra√ßage : tu peux voir tout le d√©roul√© d‚Äôune requ√™te (prompts, r√©ponses, appels API, embeddings, agents, outils utilis√©s).
- √âvaluation : comparer diff√©rentes versions de prompts ou mod√®les avec des m√©triques quantitatives/qualitatives.
- Monitoring : suivre les performances, la latence, les erreurs, les co√ªts d‚ÄôAPI, etc.
- Collaboration : partager des runs et workflows avec ton √©quipe.
- C‚Äôest donc plut√¥t un √©quivalent de ‚ÄúDataDog‚Äù ou ‚ÄúSentry‚Äù, mais pour les applications LLM.

https://python.langchain.com/docs/integrations/retrievers/

## [Usage of hosted gpu on GCP or other providers](https://python.langchain.com/docs/integrations/text_embedding/self-hosted/)

Turn your APP on locally and leverage the power of gpu hosted by providers.


## Providers and different possibilities

[Google](https://python.langchain.com/docs/integrations/providers/google/)
[Google VectorSearch](https://python.langchain.com/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
[HuggingFace](https://python.langchain.com/docs/integrations/providers/huggingface/)
[Mistral](https://python.langchain.com/docs/integrations/providers/mistralai/)

[Mistral embeddings](https://docs.mistral.ai/capabilities/embeddings/text_embeddings/) for clustering, retrieval

## There are embeddings models and chat models
### Providers solutions
Understanding embeddings GenAI model and others models ????

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-001",
        contents="What is the meaning of life?")

print(result.embeddings)
```

**[Some Use Cases](https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm/)**
-----

Supported task types
Task type	Description	Examples
SEMANTIC_SIMILARITY	Embeddings optimized to assess text similarity.	Recommendation systems, duplicate detection
CLASSIFICATION	Embeddings optimized to classify texts according to preset labels.	Sentiment analysis, spam detection
CLUSTERING	Embeddings optimized to cluster texts based on their similarities.	Document organization, market research, anomaly detection
RETRIEVAL_DOCUMENT	Embeddings optimized for document search.	Indexing articles, books, or web pages for search.
RETRIEVAL_QUERY	Embeddings optimized for general search queries. Use RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for documents to be retrieved.	Custom search
CODE_RETRIEVAL_QUERY	Embeddings optimized for retrieval of code blocks based on natural language queries. Use CODE_RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for code blocks to be retrieved.	Code suggestions and search
QUESTION_ANSWERING	Embeddings for questions in a question-answering system, optimized for finding documents that answer the question. Use QUESTION_ANSWERING for questions; RETRIEVAL_DOCUMENT for documents to be retrieved.	Chatbox
FACT_VERIFICATION	Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement. Use FACT_VERIFICATION for the target text; RETRIEVAL_DOCUMENT for documents to be retrieved	Automated fact-ch
****


[**Antropic model usages**](https://pypi.org/project/langchain-anthropic/)

[Understanding anthropic model integrations](https://python.langchain.com/docs/integrations/chat/anthropic/)

**Chat Models**
```
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, HumanMessage

model = ChatAnthropic(model="claude-3-opus-20240229", temperature=0, max_tokens=1024)
message = HumanMessage(content="What is the capital of France?")
```
Autre mod√®le de chat:
-
Others ChatVertexAI



**LLMs**
```
from langchain_anthropic import AnthropicLLM

model = AnthropicLLM(model="claude-2.1", temperature=0, max_tokens=1024)
response = model.invoke("The best restaurant in San Francisco is: ")
```

La ‚Äútemp√©rature‚Äù : √† quoi √ßa sert ?

La temp√©rature dans les mod√®les de langage est un hyperparam√®tre qui contr√¥le le degr√© de ‚Äúrandomness‚Äù / diversit√© / cr√©ativit√© dans la g√©n√©ration du texte.
o√π
ùëá
T est la temp√©rature.

Si
ùëá
=
1
T=1, on utilise les probabilit√©s ‚Äútelles quelles‚Äù.

Si
ùëá
<
1
T<1 (ex : 0.2, 0.3), on ‚Äú√©tire‚Äù les diff√©rences : les probabilit√©s √©lev√©es deviennent encore plus √©lev√©es, les faibles deviennent plus faibles ‚Üí le mod√®le est plus d√©terministe (il va souvent choisir les mots les plus probables).

Si
ùëá
>
1
T>1, on ‚Äúaplatit‚Äù les diff√©rences : cela donne plus de chance aux mots moins probables d‚Äô√™tre choisis ‚Üí sorties plus ¬´ cr√©atives ¬ª, moins pr√©visibles.

Effets pratiques

Temp√©rature basse (ex : 0.0 √† 0.3 / 0.5) : id√©al pour des t√¢ches o√π tu veux de la coh√©rence, de la pr√©cision, moins de ‚Äúd√©rives‚Äù (ex : r√©ponses factuelles, r√©sum√©, QA).

Temp√©rature plus √©lev√©e (ex : 0.7, 1.0, voire >1) : utile si tu veux de la diversit√©, de la cr√©ativit√© (√©criture, dialogues, brainstorming).

Une temp√©rature trop √©lev√©e peut rendre le texte incoh√©rent ou erratique.

## Global Overview
[See an overview topic around LLM](https://docs.mistral.ai/guides/observability/#model)

## Mistral: all available [models](https://docs.mistral.ai/getting-started/models/models_overview/)

**[Google Search](https://ai.google.dev/gemini-api/docs/live) Use case as built-in tools**

```

# Gemini can execute a Google search and use the results to ground its responses:

from langchain_google_vertexai import ChatVertexAI

llm = ChatVertexAI(model="gemini-2.5-flash").bind_tools([{"google_search": {}}])

response = llm.invoke("What is today's news?")
```

or

**[Code execution]()**

```
from langchain_google_vertexai import ChatVertexAI

llm = ChatVertexAI(model="gemini-2.5-flash").bind_tools([{"code_execution": {}}])

response = llm.invoke("What is 3^3?")
```
An other Useful resource for [grounding with google search](https://ai.google.dev/gemini-api/docs/google-search).

## **Prompt caching**

## Message Histories

## [LangSmith](https://docs.langchain.com/langsmith/home) for tracking langchain workflow : Monitor and evaluate LLM App

## LangGraph : Build effective Agentic System

## [Langchain Agent](https://python.langchain.com/docs/integrations/tools/gradio_tools/#related)

## [LangChain on GCP cloud](https://cloud.google.com/use-cases/langchain?hl=fr)

## Frontend : Gradio and streamlit

[**Streamlit**](https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/)

[**Gradio**](https://python.langchain.com/docs/integrations/tools/gradio_tools/#related)

### Utilisation de solution en local
LM Studio se concentre sur la facilit√© d‚Äôusage, OpenLLM sur la flexibilit√© (d√©ploiement, MLOps, etc.).

---
Thanks to Khuyen Tran
